import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import List, Union, Optional

class PaperAttention(nn.Module):
    def init(self, lstm_units):
        super(PaperAttention, self).init()
        lstm_output_dim = lstm_units * 2
        self.attention_Wa = nn.Linear(lstm_output_dim, lstm_output_dim)
        self.attention_v = nn.Linear(lstm_output_dim, 1, bias=False)

    def forward(self, bilstm_output):
        mu_w = torch.tanh(self.attention_Wa(bilstm_output))
        scores = self.attention_v(mu_w)
        alpha_w = F.softmax(scores, dim=1)
        context_vector = torch.sum(alpha_w * bilstm_output, dim=1)
        return context_vector


class OriginalHybridModel(nn.Module):
    def init(self, vocab_size, embedding_dim, lstm_units, cnn_filters,
                 cnn_kernel_size, dense_hidden_units):
        super(OriginalHybridModel, self).init()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.cnn_layer = nn.Conv1d(in_channels=embedding_dim,
                                   out_channels=cnn_filters,
                                   kernel_size=cnn_kernel_size)
        self.cnn_pool = nn.AdaptiveMaxPool1d(1)
        self.bilstm = nn.LSTM(input_size=embedding_dim,
                              hidden_size=lstm_units,
                              num_layers=1,
                              batch_first=True,
                              bidirectional=True)
        self.attention = PaperAttention(lstm_units)
        lstm_output_dim = lstm_units * 2
        self.fc1 = nn.Linear(cnn_filters + lstm_output_dim, dense_hidden_units)
        self.fc2 = nn.Linear(dense_hidden_units, 1)

    def forward(self, text_input):
        embedded_words = self.embedding(text_input)

        cnn_input = embedded_words.permute(0, 2, 1)
        cnn_convolved = F.relu(self.cnn_layer(cnn_input))
        cnn_pooled = self.cnn_pool(cnn_convolved)
        cnn_output = torch.squeeze(cnn_pooled, dim=-1)

        bilstm_output, _ = self.bilstm(embedded_words)
        context_vector = self.attention(bilstm_output)

        merged = torch.cat((cnn_output, context_vector), dim=1)
        hidden = F.relu(self.fc1(merged))
        output = self.fc2(hidden)
        return output
